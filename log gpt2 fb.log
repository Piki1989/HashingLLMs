/Users/michalpikus/Projects/HashingLLMs/.venv/bin/python /Users/michalpikus/Projects/HashingLLMs/HashingMultipleLLMs.py

Running gpt2 with orginal embeddings...
/Users/michalpikus/Projects/HashingLLMs/.venv/lib/python3.13/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
  0%|          | 0/250 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [01:06<00:00,  3.77it/s]
  0%|          | 0/25 [00:00<?, ?it/s]
  8%|â–Š         | 2/25 [00:00<00:01, 14.66it/s]
 16%|â–ˆâ–Œ        | 4/25 [00:00<00:02,  8.71it/s]
 24%|â–ˆâ–ˆâ–       | 6/25 [00:00<00:02,  7.69it/s]
 28%|â–ˆâ–ˆâ–Š       | 7/25 [00:00<00:02,  7.37it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:01<00:02,  7.18it/s]
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:01<00:02,  7.02it/s]
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:01<00:02,  6.92it/s]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:01<00:02,  6.85it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:01<00:01,  6.79it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:01<00:01,  6.74it/s]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:01<00:01,  6.72it/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:02<00:01,  6.69it/s]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:02<00:01,  6.65it/s]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:02<00:01,  6.65it/s]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:02<00:01,  6.64it/s]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:02<00:00,  6.64it/s]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:02<00:00,  6.63it/s]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:02<00:00,  6.63it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:03<00:00,  6.61it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:03<00:00,  6.61it/s]
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:03<00:00,  6.61it/s]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [01:11<00:00,  3.77it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:03<00:00,  6.66it/s]
                                               {'eval_loss': 0.42700183391571045, 'eval_runtime': 3.7898, 'eval_samples_per_second': 52.774, 'eval_steps_per_second': 6.597, 'epoch': 1.0}
{'train_runtime': 72.1801, 'train_samples_per_second': 13.854, 'train_steps_per_second': 3.464, 'train_loss': 0.535178466796875, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [01:12<00:00,  3.46it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:03<00:00,  6.79it/s]

Model: gpt2 (orginal)
Eval Loss: 0.4270
Perplexity: 1.53
Training Time: 72.23 sec


Running gpt2 with hash embeddings...
Applying Hash Trick Embeddings...
/Users/michalpikus/Projects/HashingLLMs/.venv/lib/python3.13/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [02:05<00:00,  2.00it/s]
  0%|          | 0/25 [00:00<?, ?it/s]
  8%|â–Š         | 2/25 [00:00<00:06,  3.33it/s]
 12%|â–ˆâ–        | 3/25 [00:01<00:09,  2.38it/s]
 16%|â–ˆâ–Œ        | 4/25 [00:01<00:10,  2.07it/s]
 20%|â–ˆâ–ˆ        | 5/25 [00:02<00:10,  1.92it/s]
 24%|â–ˆâ–ˆâ–       | 6/25 [00:02<00:10,  1.85it/s]
 28%|â–ˆâ–ˆâ–Š       | 7/25 [00:03<00:10,  1.79it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:04<00:09,  1.77it/s]
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:04<00:09,  1.75it/s]
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:05<00:08,  1.75it/s]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:05<00:08,  1.74it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:06<00:07,  1.74it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:07<00:06,  1.73it/s]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:07<00:06,  1.72it/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:08<00:05,  1.74it/s]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:08<00:05,  1.73it/s]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:09<00:04,  1.73it/s]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:09<00:04,  1.73it/s]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:10<00:03,  1.72it/s]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:11<00:02,  1.73it/s]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:11<00:02,  1.73it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:12<00:01,  1.71it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:12<00:01,  1.71it/s]
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:13<00:00,  1.71it/s]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [02:20<00:00,  2.00it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:14<00:00,  1.71it/s]
                                               {'eval_loss': 0.9566993713378906, 'eval_runtime': 14.655, 'eval_samples_per_second': 13.647, 'eval_steps_per_second': 1.706, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [02:21<00:00,  1.76it/s]
{'train_runtime': 141.6808, 'train_samples_per_second': 7.058, 'train_steps_per_second': 1.765, 'train_loss': 1.0377647705078126, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:14<00:00,  1.76it/s]

Model: gpt2 (hash)
Eval Loss: 0.9567
Perplexity: 2.60
Training Time: 141.80 sec


Running gpt2 with one_hot embeddings...
Applying One-Hot Trick Embeddings...
/Users/michalpikus/Projects/HashingLLMs/.venv/lib/python3.13/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [01:15<00:00,  3.29it/s]
  0%|          | 0/25 [00:00<?, ?it/s]
  8%|â–Š         | 2/25 [00:00<00:02,  8.52it/s]
 12%|â–ˆâ–        | 3/25 [00:00<00:03,  6.75it/s]
 16%|â–ˆâ–Œ        | 4/25 [00:00<00:03,  6.09it/s]
 20%|â–ˆâ–ˆ        | 5/25 [00:00<00:03,  5.77it/s]
 24%|â–ˆâ–ˆâ–       | 6/25 [00:00<00:03,  5.58it/s]
 28%|â–ˆâ–ˆâ–Š       | 7/25 [00:01<00:03,  5.44it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:01<00:03,  5.37it/s]
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:01<00:03,  5.33it/s]
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:01<00:02,  5.30it/s]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:01<00:02,  5.25it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:02<00:02,  5.24it/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:02<00:02,  5.24it/s]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:02<00:02,  5.23it/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:02<00:01,  5.24it/s]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:02<00:01,  5.23it/s]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:03<00:01,  5.23it/s]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:03<00:01,  5.23it/s]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:03<00:01,  5.20it/s]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:03<00:00,  5.21it/s]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:03<00:00,  5.22it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:04<00:00,  5.22it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:04<00:00,  5.22it/s]
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:04<00:00,  5.22it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.25it/s]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [01:21<00:00,  3.29it/s]
                                               {'eval_loss': 0.9928022623062134, 'eval_runtime': 4.9123, 'eval_samples_per_second': 40.714, 'eval_steps_per_second': 5.089, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [01:23<00:00,  3.00it/s]
{'train_runtime': 83.2293, 'train_samples_per_second': 12.015, 'train_steps_per_second': 3.004, 'train_loss': 1.0158673095703126, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:04<00:00,  5.30it/s]

Model: gpt2 (one_hot)
Eval Loss: 0.9928
Perplexity: 2.70
Training Time: 83.34 sec


Running facebook/opt-1.3b with orginal embeddings...
/Users/michalpikus/Projects/HashingLLMs/.venv/lib/python3.13/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [08:04<00:00,  1.89s/it]
  0%|          | 0/25 [00:00<?, ?it/s]
  8%|â–Š         | 2/25 [00:01<00:11,  1.97it/s]
 12%|â–ˆâ–        | 3/25 [00:02<00:15,  1.39it/s]
 16%|â–ˆâ–Œ        | 4/25 [00:03<00:17,  1.21it/s]
 20%|â–ˆâ–ˆ        | 5/25 [00:04<00:17,  1.11it/s]
 24%|â–ˆâ–ˆâ–       | 6/25 [00:05<00:17,  1.06it/s]
 28%|â–ˆâ–ˆâ–Š       | 7/25 [00:06<00:17,  1.03it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:07<00:17,  1.00s/it]
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:08<00:16,  1.02s/it]
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:09<00:15,  1.04s/it]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:10<00:14,  1.05s/it]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:11<00:13,  1.06s/it]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:12<00:12,  1.08s/it]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:13<00:11,  1.08s/it]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:14<00:10,  1.09s/it]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:15<00:09,  1.10s/it]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:17<00:08,  1.11s/it]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:18<00:07,  1.11s/it]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:19<00:06,  1.12s/it]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:20<00:05,  1.13s/it]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:21<00:04,  1.13s/it]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:22<00:03,  1.13s/it]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:23<00:02,  1.14s/it]
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:25<00:01,  1.14s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [08:44<00:00,  1.89s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:26<00:00,  1.15s/it]
                                               {'eval_loss': 0.36680206656455994, 'eval_runtime': 27.5084, 'eval_samples_per_second': 7.27, 'eval_steps_per_second': 0.909, 'epoch': 1.0}
{'train_runtime': 537.3323, 'train_samples_per_second': 1.861, 'train_steps_per_second': 0.465, 'train_loss': 0.5207825317382813, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [08:57<00:00,  2.15s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:26<00:00,  1.06s/it]

Model: facebook/opt-1.3b (orginal)
Eval Loss: 0.3668
Perplexity: 1.44
Training Time: 537.40 sec


Running facebook/opt-1.3b with hash embeddings...
Applying Hash Trick Embeddings...
/Users/michalpikus/Projects/HashingLLMs/.venv/lib/python3.13/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [08:32<00:00,  2.05s/it]
  0%|          | 0/25 [00:00<?, ?it/s]
  8%|â–Š         | 2/25 [00:01<00:16,  1.37it/s]
 12%|â–ˆâ–        | 3/25 [00:02<00:22,  1.04s/it]
 16%|â–ˆâ–Œ        | 4/25 [00:04<00:25,  1.19s/it]
 20%|â–ˆâ–ˆ        | 5/25 [00:05<00:25,  1.29s/it]
 24%|â–ˆâ–ˆâ–       | 6/25 [00:07<00:25,  1.35s/it]
 28%|â–ˆâ–ˆâ–Š       | 7/25 [00:08<00:25,  1.39s/it]
 32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:10<00:24,  1.41s/it]
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:11<00:23,  1.44s/it]
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:13<00:21,  1.46s/it]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:14<00:20,  1.47s/it]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:16<00:19,  1.48s/it]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:17<00:17,  1.48s/it]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:19<00:16,  1.49s/it]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:20<00:14,  1.50s/it]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:22<00:13,  1.50s/it]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:23<00:12,  1.50s/it]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:25<00:10,  1.50s/it]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:26<00:09,  1.51s/it]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:28<00:07,  1.51s/it]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:29<00:06,  1.51s/it]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:31<00:04,  1.51s/it]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:32<00:03,  1.51s/it]
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:34<00:01,  1.52s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [09:22<00:00,  2.05s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:36<00:00,  1.52s/it]
                                               {'eval_loss': 0.8734051585197449, 'eval_runtime': 37.8072, 'eval_samples_per_second': 5.29, 'eval_steps_per_second': 0.661, 'epoch': 1.0}
{'train_runtime': 572.7926, 'train_samples_per_second': 1.746, 'train_steps_per_second': 0.436, 'train_loss': 0.9047498779296875, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [09:32<00:00,  2.29s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:36<00:00,  1.46s/it]

Model: facebook/opt-1.3b (hash)
Eval Loss: 0.8734
Perplexity: 2.40
Training Time: 573.14 sec


Running facebook/opt-1.3b with one_hot embeddings...
Applying One-Hot Trick Embeddings...
/Users/michalpikus/Projects/HashingLLMs/.venv/lib/python3.13/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [08:14<00:00,  1.98s/it]
  0%|          | 0/25 [00:00<?, ?it/s]
  8%|â–Š         | 2/25 [00:01<00:13,  1.69it/s]
 12%|â–ˆâ–        | 3/25 [00:02<00:17,  1.24it/s]
 16%|â–ˆâ–Œ        | 4/25 [00:03<00:19,  1.09it/s]
 20%|â–ˆâ–ˆ        | 5/25 [00:04<00:19,  1.01it/s]
 24%|â–ˆâ–ˆâ–       | 6/25 [00:05<00:19,  1.03s/it]
 28%|â–ˆâ–ˆâ–Š       | 7/25 [00:06<00:19,  1.07s/it]
 32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:07<00:18,  1.09s/it]
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:09<00:17,  1.12s/it]
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:10<00:17,  1.15s/it]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:11<00:16,  1.16s/it]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:12<00:15,  1.17s/it]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:13<00:14,  1.18s/it]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:15<00:13,  1.20s/it]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:16<00:12,  1.21s/it]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:17<00:10,  1.21s/it]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:18<00:09,  1.22s/it]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:20<00:08,  1.23s/it]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:21<00:07,  1.23s/it]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:22<00:06,  1.24s/it]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:23<00:04,  1.24s/it]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:25<00:03,  1.25s/it]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:26<00:02,  1.26s/it]
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:27<00:01,  1.26s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [08:58<00:00,  1.98s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:29<00:00,  1.26s/it]
                                               {'eval_loss': 0.9774861335754395, 'eval_runtime': 30.5871, 'eval_samples_per_second': 6.539, 'eval_steps_per_second': 0.817, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [09:11<00:00,  2.20s/it]
{'train_runtime': 551.0361, 'train_samples_per_second': 1.815, 'train_steps_per_second': 0.454, 'train_loss': 1.1597449951171874, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:29<00:00,  1.16s/it]

Model: facebook/opt-1.3b (one_hot)
Eval Loss: 0.9775
Perplexity: 2.66
Training Time: 551.28 sec
